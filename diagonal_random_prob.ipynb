{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import numpy as np\n",
    "from create_data import create_sparse_data\n",
    "import torch\n",
    "from scipy.optimize import minimize\n",
    "import os\n",
    "\n",
    "def calc_probabilities(file_name, mean_seed_mode=False, seed_vec=[]):\n",
    "    # Load the data\n",
    "    df = pd.read_csv(file_name)\n",
    "    # print(df)\n",
    "    # data_seed_values = [0, 300, 600, 900]\n",
    "    if len(seed_vec)==0:\n",
    "        data_seed_values = df['data_seed'].unique()\n",
    "    else:\n",
    "        data_seed_values = seed_vec\n",
    "    depth_vec = df['depth'].unique()\n",
    "    loss_thr_vec = [(0, 1)]\n",
    "    res_df = pd.DataFrame()\n",
    "    for loss_thr_ind, loss_thr in enumerate(loss_thr_vec):\n",
    "        for depth in depth_vec:\n",
    "            for dist in [\"Normal\", \"Normal_uv\", \"Laplace\"]:\n",
    "                if depth>2 and dist!=\"Normal_uv\":\n",
    "                    continue\n",
    "                p2_vec = []\n",
    "                # if mean_seed_mode:\n",
    "                #     df_dist = df[(df['dist']==dist) & (df['depth']==depth) & (df['loss_min_thr']==loss_thr[0]) & (df['loss_max_thr']==loss_thr[1]) & (df['status']=='Success')]\n",
    "                #     if len(df_dist)>0:\n",
    "                #             p1 = len(df_dist[df_dist['test_acc']>0])/((df_dist['iter']+1).sum())\n",
    "                #             p2 = (df_dist[df_dist['test_acc']>=0]['test_acc']).mean()\n",
    "                #             cur_res_df = pd.DataFrame({'dist': dist, 'depth': depth, 'p1': p1, 'p2': p2}, index=[0])\n",
    "                #             res_df = pd.concat([res_df, cur_res_df], axis=0)\n",
    "                # else:\n",
    "                seed_counter = 0\n",
    "                for seed_ind, seed in enumerate(data_seed_values):\n",
    "                    df_dist = df[(df['dist']==dist) & (df['data_seed']==seed) & (df['depth']==depth) & (df['loss_min_thr']==loss_thr[0]) & (df['loss_max_thr']==loss_thr[1]) & (df['status']=='Success')]                        \n",
    "                    if len(df_dist)>0:\n",
    "                        p1 = len(df_dist[df_dist['test_acc']>0])/((df_dist['iter']+1).sum())\n",
    "                        p2 = (df_dist[df_dist['test_acc']>=0]['test_acc']).mean()\n",
    "                        p2_vec.append(p2)\n",
    "                        seed_counter+=1\n",
    "                    if not mean_seed_mode:\n",
    "                        cur_res_df = pd.DataFrame({'seed': seed, 'dist': dist, 'depth': depth, 'p1': p1, 'p2': p2}, index=[0])\n",
    "                        res_df = pd.concat([res_df, cur_res_df], axis=0)\n",
    "                if mean_seed_mode and seed_counter>0:\n",
    "                    p2_mean = np.mean(p2_vec)\n",
    "                    p2_std = np.std(p2_vec)\n",
    "                    cur_res_df = pd.DataFrame({'dist': dist, 'depth': depth, 'p1': p1, 'p2_mean': p2_mean, 'p2_std': p2_std, 'seed_counter': seed_counter}, index=[0])\n",
    "                    res_df = pd.concat([res_df, cur_res_df], axis=0)\n",
    "    return res_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def constraint(v, x, y):\n",
    "    return np.multiply(y, np.matmul(x, v.reshape(-1, 1)).squeeze())-1\n",
    "\n",
    "\n",
    "def solver(x, y, w_0, obj='Q', optim_tol=1e-5):\n",
    "    x0 = (w_0).reshape(-1, )\n",
    "\n",
    "    cons = {'type': 'ineq', 'fun': lambda v: constraint(v, x, y)}\n",
    "\n",
    "    if obj == 'L1':\n",
    "        objective = lambda v: np.linalg.norm(v.squeeze(), ord=1)\n",
    "    elif obj == 'L2':\n",
    "        objective = lambda v: np.linalg.norm(v.squeeze(), ord=2)\n",
    "    else:\n",
    "        raise ValueError('objective not supported.')\n",
    "\n",
    "    sol = minimize(\n",
    "        fun=objective,\n",
    "        x0=x0,\n",
    "        constraints=cons,\n",
    "        tol=optim_tol,\n",
    "        method='SLSQP',\n",
    "        options={\n",
    "            'maxiter': 1000000,\n",
    "            'disp': False\n",
    "        }\n",
    "    )\n",
    "    is_failed = (not sol.success)\n",
    "    if is_failed:\n",
    "        raise RuntimeError('Minimization Failed.')\n",
    "\n",
    "    return sol.x\n",
    "\n",
    "def calc_test_acc(x_test, y_test, w):\n",
    "    y_hat = torch.matmul(x_test, torch.tensor(w).float())\n",
    "    predicton = (y_hat*y_test>0).float()\n",
    "    test_acc = torch.sum(predicton)/len(y_test)\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_baselines(file_name):\n",
    "    # Load the results\n",
    "    df = pd.read_csv(file_name)\n",
    "    N_train = df['N_train'].unique()[0]\n",
    "    N_test = df['N_test'].unique()[0]\n",
    "    d = df['d'].unique()[0]\n",
    "    r = df['r'].unique()[0]\n",
    "    mu = df['mu'].unique()[0]\n",
    "    dataset = df['dataset'].unique()[0]\n",
    "    data_seed_values = df['data_seed'].unique()\n",
    "    test_acc_across_seeds_l0 = []\n",
    "    test_acc_across_seeds_l1 = []\n",
    "    for seed_ind, seed in enumerate(data_seed_values):\n",
    "        x_train, y_train, x_test, y_test = create_sparse_data(n_train=N_train, n_test=N_test, r=r, dataset=dataset, normalized_flag=False, seed=seed, mu=mu, d=d)\n",
    "        x_train, y_train, x_test, y_test = x_train.cpu(), y_train.cpu(), x_test.cpu(), y_test.cpu() \n",
    "        if r==1:\n",
    "            # l0 baseline\n",
    "            test_acc_vec_l0 = []\n",
    "            for ind in range(d):\n",
    "                w_L0 = np.zeros((d))\n",
    "                w_L0[ind] = 1\n",
    "                train_acc = calc_test_acc(x_train, y_train, w_L0)\n",
    "                if train_acc==1.0:\n",
    "                    test_acc_vec_l0.append(calc_test_acc(x_test, y_test, w_L0))\n",
    "            test_acc_l0 = test_acc_vec_l0[np.random.randint(len(test_acc_vec_l0))]\n",
    "            # save result in csv\n",
    "            df_res = pd.DataFrame({'N_train': N_train, 'N_test': N_test, 'd': d, 'r': r, 'mu': mu, 'dataset': dataset, 'data_seed': seed, 'type': 'L0', 'test_acc': test_acc_l0.detach().cpu().numpy()}, index=[0])\n",
    "            baselines_file_name = f\"baselines.csv\"\n",
    "            if not os.path.exists(baselines_file_name):\n",
    "                df_res.to_csv(baselines_file_name, mode='w', index=False)\n",
    "            # Check if file is empty\n",
    "            elif os.stat(baselines_file_name).st_size == 0:\n",
    "                df_res.to_csv(baselines_file_name, mode='a', index=False)\n",
    "            else:\n",
    "                df_res.to_csv(baselines_file_name, mode='a', header=False, index=False)\n",
    "        \n",
    "        # l1 baseline\n",
    "        w_0 = np.random.randn(d, 1)\n",
    "        # Create l1 baseline\n",
    "        w_L1 = solver(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        w_0=w_0,\n",
    "        obj='L1',\n",
    "        optim_tol=1e-8\n",
    "        )\n",
    "        test_acc_across_seeds_l1.append(calc_test_acc(x_test, y_test, w_L1))\n",
    "        # save result in csv\n",
    "        df_res = pd.DataFrame({'N_train': N_train, 'N_test': N_test, 'd': d, 'r': r, 'mu': mu, 'dataset': dataset, 'data_seed': seed, 'type': 'L1', 'test_acc': calc_test_acc(x_test, y_test, w_L1).detach().cpu().numpy()}, index=[0])\n",
    "        baselines_file_name = f\"baselines.csv\"\n",
    "        if not os.path.exists(baselines_file_name):\n",
    "            df_res.to_csv(baselines_file_name, mode='w', index=False)\n",
    "        # Check if file is empty\n",
    "        elif os.stat(baselines_file_name).st_size == 0:\n",
    "            df_res.to_csv(baselines_file_name, mode='a', index=False)\n",
    "        else:\n",
    "            df_res.to_csv(baselines_file_name, mode='a', header=False, index=False)\n",
    "\n",
    "        if seed_ind%10==0:\n",
    "            print(f\"seed={seed}, test_acc={test_acc_across_seeds_l1[-1]}\")\n",
    "    print(test_acc_across_seeds_l1)\n",
    "    return np.mean(test_acc_across_seeds_l0), np.mean(test_acc_across_seeds_l1) #, np.std(test_acc_across_seeds)\n",
    "\n",
    "\n",
    "def load_baselines(res_file_name, baselines_file_name):\n",
    "    # Load the results\n",
    "    df_res = pd.read_csv(res_file_name)\n",
    "    df_baselines = pd.read_csv(baselines_file_name)\n",
    "\n",
    "\n",
    "    N_train = df_res['N_train'].unique()[0]\n",
    "    N_test = df_res['N_test'].unique()[0]\n",
    "    d = df_res['d'].unique()[0]\n",
    "    r = df_res['r'].unique()[0]\n",
    "    mu = df_res['mu'].unique()[0]\n",
    "    dataset = df_res['dataset'].unique()[0]\n",
    "    l1_baseline = df_baselines.loc[(df_baselines['type']=='L1') & (df_baselines['N_train']==N_train) & (df_baselines['N_test']==N_test) & (df_baselines['d']==d) & (df_baselines['r']==r) & (df_baselines['mu']==mu) & (df_baselines['dataset']==dataset)]['test_acc'].mean()\n",
    "    \n",
    "    if r==1:\n",
    "        l0_baseline = df_baselines.loc[(df_baselines['type']=='L0') & (df_baselines['N_train']==N_train) & (df_baselines['N_test']==N_test) & (df_baselines['d']==d) & (df_baselines['r']==r) & (df_baselines['mu']==mu) & (df_baselines['dataset']==dataset)]['test_acc'].mean()\n",
    "    else:\n",
    "        l0_baseline = -1\n",
    "    return l1_baseline, l0_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed=0, test_acc=0.800000011920929\n",
      "[tensor(0.8000), tensor(1.), tensor(0.9000), tensor(0.6000), tensor(1.), tensor(0.8000), tensor(0.8000), tensor(0.9000), tensor(0.6000), tensor(0.7000)]\n",
      "results_new_1005_prob_N_train_16_d_128_r_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mor/anaconda3/envs/myenv/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/mor/anaconda3/envs/myenv/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "N_train_vec = [16]\n",
    "d_vec = [128]\n",
    "r = 1\n",
    "\n",
    "res_total_df = pd.DataFrame()\n",
    "for N_train in N_train_vec:\n",
    "    for d in d_vec:\n",
    "        file_name = f\"results_new_1005_prob_N_train_{N_train}_d_{d}_r_{r}.csv\"\n",
    "        baselines_file_name = f\"baselines.csv\"\n",
    "        baselinel1, baselinel0 = calc_baselines(file_name)\n",
    "        baselinel1, baselinel0 = load_baselines(file_name, baselines_file_name)\n",
    "        try:\n",
    "            print(file_name)\n",
    "            res_df = calc_probabilities(file_name, mean_seed_mode=True)\n",
    "            res_df = pd.concat([pd.DataFrame({'N_train': N_train, 'd': d, 'r': r}, index=[0]), res_df], axis=1)\n",
    "            res_total_df = pd.concat([res_total_df, res_df], axis=0)\n",
    "            res_total_df = pd.concat([res_total_df, pd.DataFrame({'N_train': N_train, 'd': d, 'r': r, 'dist': 'L0', 'p2_mean': baselinel0}, index=[0])], axis=0)\n",
    "            res_total_df = pd.concat([res_total_df, pd.DataFrame({'N_train': N_train, 'd': d, 'r': r, 'dist': 'L1', 'p2_mean': baselinel1}, index=[0])], axis=0)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_total_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train_vec = [12]\n",
    "d_vec = [128]\n",
    "r = 1\n",
    "\n",
    "res_total_df = pd.DataFrame()\n",
    "for N_train in N_train_vec:\n",
    "    for d in d_vec:\n",
    "        file_name = f\"results_new_prob_N_train_{N_train}_d_{d}.csv\"\n",
    "        baselines_file_name = f\"baselines.csv\"\n",
    "        # baseline = calc_baselines(file_name)\n",
    "        baselinel1, baselinel0 = load_baselines(file_name, baselines_file_name)\n",
    "        try:\n",
    "            print(file_name)\n",
    "            res_df = calc_probabilities(file_name, mean_seed_mode=True)\n",
    "            res_df = pd.concat([pd.DataFrame({'N_train': N_train, 'd': d, 'r': r}, index=[0]), res_df], axis=1)\n",
    "            res_total_df = pd.concat([res_total_df, res_df], axis=0)\n",
    "            res_total_df = pd.concat([res_total_df, pd.DataFrame({'N_train': N_train, 'd': d, 'r': r, 'dist': 'L0', 'p2_mean': baselinel0}, index=[0])], axis=0)\n",
    "            res_total_df = pd.concat([res_total_df, pd.DataFrame({'N_train': N_train, 'd': d, 'r': r, 'dist': 'L1', 'p2_mean': baselinel1}, index=[0])], axis=0)\n",
    "        except:\n",
    "            pass\n",
    "\t\t\t\n",
    "N_train_vec = [12]\n",
    "d_vec = [128]\n",
    "r = 5\n",
    "\n",
    "res_total_df = pd.DataFrame()\n",
    "for N_train in N_train_vec:\n",
    "    for d in d_vec:\n",
    "        file_name = f\"results_new_prob_N_train_{N_train}_d_{d}_r_{r}.csv\"\n",
    "        # baseline = calc_baselines(file_name)\n",
    "        baselines_file_name = f\"baselines.csv\"\n",
    "        baselinel1, _ = load_baselines(file_name, baselines_file_name)\n",
    "        try:\n",
    "            print(file_name)\n",
    "            res_df = calc_probabilities(file_name, mean_seed_mode=True)\n",
    "            res_df = pd.concat([pd.DataFrame({'N_train': N_train, 'd': d, 'r': r}, index=[0]), res_df], axis=1)\n",
    "            res_total_df = pd.concat([res_total_df, res_df], axis=0)\n",
    "            res_total_df = pd.concat([res_total_df, pd.DataFrame({'N_train': N_train, 'd': d, 'r': r, 'dist': 'L1', 'p2_mean': baselinel1}, index=[0])], axis=0)\n",
    "        except:\n",
    "            pass\n",
    "\t\t\t\n",
    "N_train_vec = [12]\n",
    "d_vec = [128]\n",
    "r = 2\n",
    "\n",
    "res_total_df = pd.DataFrame()\n",
    "for N_train in N_train_vec:\n",
    "    for d in d_vec:\n",
    "        file_name = f\"results_new_prob_N_train_{N_train}_d_{d}_r_{r}.csv\"\n",
    "        # baseline = calc_baselines(file_name)\n",
    "        baselines_file_name = f\"baselines.csv\"\n",
    "        baselinel1, _ = load_baselines(file_name, baselines_file_name)\n",
    "        try:\n",
    "            print(file_name)\n",
    "            res_df = calc_probabilities(file_name, mean_seed_mode=True)\n",
    "            res_df = pd.concat([pd.DataFrame({'N_train': N_train, 'd': d, 'r': r}, index=[0]), res_df], axis=1)\n",
    "            res_total_df = pd.concat([res_total_df, res_df], axis=0)\n",
    "            res_total_df = pd.concat([res_total_df, pd.DataFrame({'N_train': N_train, 'd': d, 'r': r, 'dist': 'L1', 'p2_mean': baselinel1}, index=[0])], axis=0)\n",
    "        except:\n",
    "            pass\n",
    "\t\t\t\n",
    "## Generalization Vs Depth\n",
    "\n",
    "N_train_vec = [12]\n",
    "d_vec = [128]\n",
    "r = 1\n",
    "\n",
    "res_total_df = pd.DataFrame()\n",
    "for N_train in N_train_vec:\n",
    "    for d in d_vec:\n",
    "        file_name = f\"results_new_prob_N_train_{N_train}_d_{d}_r_{r}.csv\"\n",
    "        # baseline = calc_baselines(file_name)\n",
    "        baselines_file_name = f\"baselines.csv\"\n",
    "        baselinel1, _ = load_baselines(file_name, baselines_file_name)\n",
    "        # try:\n",
    "        print(file_name)\n",
    "        res_df = calc_probabilities(file_name, mean_seed_mode=True)\n",
    "        res_df = pd.concat([pd.DataFrame({'N_train': N_train, 'd': d, 'r': r}, index=[0]), res_df], axis=1)\n",
    "        res_total_df = pd.concat([res_total_df, res_df], axis=0)\n",
    "        res_total_df = pd.concat([res_total_df, pd.DataFrame({'N_train': N_train, 'd': d, 'r': r, 'dist': 'L1', 'p2_mean': baselinel1}, index=[0])], axis=0)\n",
    "        # except:\n",
    "            # pass\n",
    "\t\t\t\n",
    "## r=2\n",
    "\n",
    "N_train_vec = [12]\n",
    "d_vec = [128]\n",
    "r = 2\n",
    "\n",
    "res_total_df = pd.DataFrame()\n",
    "for N_train in N_train_vec:\n",
    "    for d in d_vec:\n",
    "        file_name = f\"results_new_prob_N_train_{N_train}_d_{d}_r_{r}.csv\"\n",
    "        # baseline = calc_baselines(file_name)\n",
    "        baselines_file_name = f\"baselines.csv\"\n",
    "        baselinel1, _ = load_baselines(file_name, baselines_file_name)\n",
    "        # try:\n",
    "        print(file_name)\n",
    "        res_df = calc_probabilities(file_name, mean_seed_mode=True, seed_vec=np.arange(0,10).tolist())\n",
    "        res_df = pd.concat([pd.DataFrame({'N_train': N_train, 'd': d, 'r': r}, index=[0]), res_df], axis=1)\n",
    "        res_total_df = pd.concat([res_total_df, res_df], axis=0)\n",
    "        res_total_df = pd.concat([res_total_df, pd.DataFrame({'N_train': N_train, 'd': d, 'r': r, 'dist': 'L1', 'p2_mean': baselinel1}, index=[0])], axis=0)\n",
    "        # except:\n",
    "            # pass\n",
    "\t\t\t\n",
    "res_total_df\n",
    "\n",
    "plt.plot(res_total_df[res_total_df['dist']=='Normal_uv']['depth'], res_total_df[res_total_df['dist']=='Normal_uv']['p2_mean'], 'o')\n",
    "plt.xlabel(\"Depth\")\n",
    "plt.ylabel(\"Mean Test Accuracy\")\n",
    "\n",
    "N_train_vec = [32]\n",
    "d_vec = [128]\n",
    "r = 2\n",
    "\n",
    "res_total_df = pd.DataFrame()\n",
    "for N_train in N_train_vec:\n",
    "    for d in d_vec:\n",
    "        file_name = f\"results_new_prob_N_train_{N_train}_d_{d}_r_{r}.csv\"\n",
    "        # baseline = calc_baselines(file_name)\n",
    "        baselines_file_name = f\"baselines.csv\"\n",
    "        baselinel1, _ = load_baselines(file_name, baselines_file_name)\n",
    "        # try:\n",
    "        print(file_name)\n",
    "        res_df = calc_probabilities(file_name, mean_seed_mode=True, seed_vec=np.arange(0,10).tolist())\n",
    "        res_df = pd.concat([pd.DataFrame({'N_train': N_train, 'd': d, 'r': r}, index=[0]), res_df], axis=1)\n",
    "        res_total_df = pd.concat([res_total_df, res_df], axis=0)\n",
    "        res_total_df = pd.concat([res_total_df, pd.DataFrame({'N_train': N_train, 'd': d, 'r': r, 'dist': 'L1', 'p2_mean': baselinel1}, index=[0])], axis=0)\n",
    "        # except:\n",
    "            # pass\n",
    "\t\t\t\n",
    "res_total_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results_new_1005_prob_N_train_16_d_128_r_1.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dist</th>\n",
       "      <th>depth</th>\n",
       "      <th>p1</th>\n",
       "      <th>p2_mean</th>\n",
       "      <th>p2_std</th>\n",
       "      <th>seed_counter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Normal_uv</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.497536</td>\n",
       "      <td>0.030176</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Normal_uv</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.675466</td>\n",
       "      <td>0.062799</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Normal_uv</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.827787</td>\n",
       "      <td>0.056976</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Normal_uv</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.872884</td>\n",
       "      <td>0.049011</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Normal_uv</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.898529</td>\n",
       "      <td>0.043779</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Normal_uv</td>\n",
       "      <td>15</td>\n",
       "      <td>0.000514</td>\n",
       "      <td>0.932552</td>\n",
       "      <td>0.033749</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Normal_uv</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000765</td>\n",
       "      <td>0.947810</td>\n",
       "      <td>0.028607</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Normal_uv</td>\n",
       "      <td>25</td>\n",
       "      <td>0.000883</td>\n",
       "      <td>0.954295</td>\n",
       "      <td>0.025231</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Normal_uv</td>\n",
       "      <td>30</td>\n",
       "      <td>0.001044</td>\n",
       "      <td>0.961099</td>\n",
       "      <td>0.023135</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Normal_uv</td>\n",
       "      <td>35</td>\n",
       "      <td>0.001235</td>\n",
       "      <td>0.965036</td>\n",
       "      <td>0.022091</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Normal_uv</td>\n",
       "      <td>40</td>\n",
       "      <td>0.001407</td>\n",
       "      <td>0.967465</td>\n",
       "      <td>0.020643</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Normal_uv</td>\n",
       "      <td>45</td>\n",
       "      <td>0.001376</td>\n",
       "      <td>0.970116</td>\n",
       "      <td>0.018958</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Normal_uv</td>\n",
       "      <td>50</td>\n",
       "      <td>0.001583</td>\n",
       "      <td>0.973139</td>\n",
       "      <td>0.018281</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        dist  depth        p1   p2_mean    p2_std  seed_counter\n",
       "0  Normal_uv      2  0.000031  0.497536  0.030176            10\n",
       "0  Normal_uv      4  0.000049  0.675466  0.062799            10\n",
       "0  Normal_uv      6  0.000081  0.827787  0.056976            10\n",
       "0  Normal_uv      8  0.000180  0.872884  0.049011            10\n",
       "0  Normal_uv     10  0.000270  0.898529  0.043779            10\n",
       "0  Normal_uv     15  0.000514  0.932552  0.033749            10\n",
       "0  Normal_uv     20  0.000765  0.947810  0.028607            10\n",
       "0  Normal_uv     25  0.000883  0.954295  0.025231            10\n",
       "0  Normal_uv     30  0.001044  0.961099  0.023135            10\n",
       "0  Normal_uv     35  0.001235  0.965036  0.022091            10\n",
       "0  Normal_uv     40  0.001407  0.967465  0.020643            10\n",
       "0  Normal_uv     45  0.001376  0.970116  0.018958            10\n",
       "0  Normal_uv     50  0.001583  0.973139  0.018281            10"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_train_vec = [16]\n",
    "d_vec = [128]\n",
    "r = 1\n",
    "\n",
    "res_total_df = pd.DataFrame()\n",
    "for N_train in N_train_vec:\n",
    "    for d in d_vec:\n",
    "        file_name = f\"results_new_1005_prob_N_train_{N_train}_d_{d}_r_{r}.csv\"\n",
    "        # baseline = calc_baselines(file_name)\n",
    "        baselines_file_name = f\"baselines.csv\"\n",
    "        baselinel1, _ = load_baselines(file_name, baselines_file_name)\n",
    "        # try:\n",
    "        print(file_name)\n",
    "        res_df = calc_probabilities(file_name, mean_seed_mode=True, seed_vec=np.arange(0,10).tolist())\n",
    "        # res_df = pd.concat([pd.DataFrame({'N_train': N_train, 'd': d, 'r': r}, index=[0]), res_df], axis=1)\n",
    "        # res_total_df = pd.concat([res_total_df, res_df], axis=0)\n",
    "        # res_total_df = pd.concat([res_total_df, pd.DataFrame({'N_train': N_train, 'd': d, 'r': r, 'dist': 'L1', 'p2_mean': baselinel1}, index=[0])], axis=0)\n",
    "        # except:\n",
    "            # pass\n",
    "\t\t\t\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
